from fastapi import Request
from fastapi.templating import Jinja2Templates
import numpy as np
import re
import requests
import csv
import pandas as pd
import sys
from model import Action, Category 
from pydantic import BaseModel
from datetime import datetime
import traceback
import os

data = Action.data_game
data_json = Action.data_json
categories = Category.category_name[:15]
favorite_list =  []
web = Jinja2Templates(directory="resources/view")
CSV_FILE = "fetch_date.csv"

def welcome(request: Request, web:Jinja2Templates):    
    return web.TemplateResponse("home.html",{"request": request})

def home(request: Request, web: Jinja2Templates, page: int = 1, limit: int = 8):
    start = (page - 1) * limit
    end = start + limit 
    chunk = data.iloc[start:end].to_dict(orient="records")
    items = {
        "request": request,
        "data_list": chunk,
        "current_page": page,
        "categories": categories
    }    
    return web.TemplateResponse("home.html", items)


def get_data_by_category(category: str):
    df = Action.get_data_category(category)
    return df

import re
import requests

class Crawler:
    def fetch(self, url: str) -> str:
        """‡∏î‡∏∂‡∏á HTML ‡∏à‡∏≤‡∏Å URL"""
        try:
            response = requests.get(url)
            response.raise_for_status()
            return response.text
        except Exception as e:
            print(f"‚ùå fetch error: {e} | URL: {url}")
            return ""

    def crawl_nav(self, web_txt: str) -> list[str]:
        """‡∏î‡∏∂‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏à‡∏≤‡∏Å navigation"""
        regex_nav = r'<li\sid="menu-item-[\d]*"\sclass="menu-[\w]*"><a\shref="([\w:/.-]*)">[\w\s]*<span\sclass="p"></span></a></li>'
        return re.findall(regex_nav, web_txt)

    def next_page(self, web_txt: str) -> str | None:
        """‡∏î‡∏∂‡∏á‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏´‡∏ô‡πâ‡∏≤‡∏ñ‡∏±‡∏î‡πÑ‡∏õ"""
        regex_next_page = r'<a\s+class="next"\s+href="([^"]+)">'
        link = re.findall(regex_next_page, web_txt)
        return link[0] if link else None

    def crawl(self, web_text: str, ids_seen: set) -> list[dict]:
        """‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÇ‡∏û‡∏™‡∏ï‡πå‡∏à‡∏≤‡∏Å‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö ‡πÅ‡∏•‡∏∞‡∏Å‡∏±‡∏ô‡∏ã‡πâ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ ids_seen"""
        posts = []
        titles = re.findall(
            r'<a\sclass="post-thumb\s"\sid="thumb-[\d]*"\shref="[\w:/.-]*"\stitle="([\w\s.]*)">',
            web_text,
        )
        ids = re.findall(r'<a\sclass="post-thumb\s"\sid="thumb-([\d]*)"', web_text)
        imgs = re.findall(r'<img\swidth="140"\sheight="140"\ssrc="([\w:/.-]*)"', web_text)
        dates = re.findall(
            r'<div\sclass="post-std\sclear-block">[\w\W]*?<div\sclass="post-date"><span\sclass="ext">([\d\s\w]+)</span></div>',
            web_text,
        )
        descriptions = re.findall(
            r'<div\sclass="post-content\sclear-block">[\w\W]*?([\w\W]+?)\s*[(]more&hellip;',
            web_text,
        )
        tag_blocks = re.findall(r'<div class="post-info">([\w\W]*?)</div>', web_text)

        for i in range(len(titles)):
            pid = ids[i].strip()
            if pid in ids_seen:
                continue
            ids_seen.add(pid)

            tags = (
                ", ".join(re.findall(r'\srel="tag"\stitle="([\w]+)', tag_blocks[i]))
                if i < len(tag_blocks)
                else []
            )

            post = {
                "ID": pid,
                "Title": titles[i].strip() if i < len(titles) else None,
                "Tags": tags,
                "Image": imgs[i].strip() if i < len(imgs) else None,
                "Date": dates[i].strip() if i < len(dates) else None,
                "Description": descriptions[i].strip() if i < len(descriptions) else None,
            }
            posts.append(post)

        return posts

    def crawl_loop(self, links_list: list[str], max_page_per_cate: int = 2) -> list[dict]:
        """
        ‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏•‡∏¥‡∏á‡∏Å‡πå‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        ‡∏Å‡∏±‡∏ô‡∏ã‡πâ‡∏≥‡∏î‡πâ‡∏ß‡∏¢ ids_seen ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
        """
        data = []
        ids_seen = set()  # ‡∏Å‡∏±‡∏ô‡∏ã‡πâ‡∏≥‡∏ó‡∏±‡πà‡∏ß‡∏ó‡∏±‡πâ‡∏á crawl
        for web_travse in links_list:
            web = self.fetch(web_travse)
            if not web:
                continue
            page = 1
            while page <= max_page_per_cate:
                data += self.crawl(web, ids_seen)
                next_page = self.next_page(web)
                if next_page:
                    web = self.fetch(next_page)
                else:
                    break
                page += 1
        print(f"‚úÖ crawl_loop: collected {len(data)} unique items")
        return data

class Export:
    def export_csv(self, data, file_name):
        if not data:
            return
        fieldnames = data[0].keys()
        with open(file_name, mode="w", newline="", encoding="utf-8") as file:
            writer = csv.DictWriter(file, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(data)


class FavoriteRequest(BaseModel):
    game_id: int
    
# ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡πà‡∏ô‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ï‡πá‡∏°‡∏ï‡∏≤‡∏° id list
def get_full_data_by_ids(ids: list):
    """
    ‡∏£‡∏±‡∏ö ids: list ‡∏Ç‡∏≠‡∏á ID (int ‡∏´‡∏£‡∏∑‡∏≠ str)
    ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÄ‡∏ï‡πá‡∏°‡∏à‡∏≤‡∏Å data_json
    ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ id ‡πÉ‡∏î match ‡∏à‡∏∞ return []
    """
    if not ids or not isinstance(ids, list):
        return []

    if not data_json:
        return []

    result = []
    seen_ids = set()
    for item in data_json:
        item_id = str(item.get("ID"))
        if item_id in ids and item_id not in seen_ids:
            result.append(item)
            seen_ids.add(item_id)

    return result


def favorite_data_export(favorite_list):
    try:
        # ‡∏•‡∏ö id ‡∏ã‡πâ‡∏≥‡∏≠‡∏≠‡∏Å‡∏Å‡πà‡∏≠‡∏ô export
        unique_favorite_list = list(dict.fromkeys(favorite_list))
        data = get_full_data_by_ids(unique_favorite_list)
        df = pd.DataFrame(data)
        df['Tags'] = df['Tags'].apply(lambda x: ', '.join(x))
        df.to_csv('favorite.csv', index=False, encoding='utf-8-sig')
        print("Exported favorite.csv successfully!")
        print(data)
        return "export successfully"
    except Exception as e:
        return f"Error during export: {e}"


def fetch_data():
    """‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÄ‡∏ß‡πá‡∏ö ‡πÅ‡∏•‡∏∞ update global data"""
    try:
        data_fetch = Crawler()
        web = data_fetch.fetch("https://oceanofgames.com/")
        links_cate = data_fetch.crawl_nav(web)
        data_update = data_fetch.crawl_loop(links_list=links_cate)

        # ‡∏≠‡∏±‡∏û‡πÄ‡∏î‡∏ï global Action.data_json + sync CSV
        game_update = Action.update_data(data_update)  # ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤ list ‡πÄ‡∏™‡∏°‡∏≠

        global data, data_json
        data = game_update or []  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô None
        data_json = Action.data_json  # ‡πÉ‡∏ä‡πâ source of truth ‡∏ï‡∏£‡∏á ‡πÜ

        print("‚úÖ Fetch success, total items:", len(data))
        return "fetching successfully"

    except Exception as e:
        import traceback
        traceback.print_exc()
        return f"‚ùå Error while fetching: {e}"

def safe_read_csv(file):
    """‡∏≠‡πà‡∏≤‡∏ô CSV ‡πÅ‡∏ö‡∏ö‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ ‡πÑ‡∏°‡πà‡πÄ‡∏à‡∏≠ EmptyDataError"""
    if not os.path.exists(file):
        return pd.DataFrame()  # ‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå
    if os.path.getsize(file) == 0:
        return pd.DataFrame()  # ‡πÑ‡∏ü‡∏•‡πå‡∏ß‡πà‡∏≤‡∏á
    try:
        return pd.read_csv(file)
    except pd.errors.EmptyDataError:
        return pd.DataFrame()

def save_fetch_time(source="manual"):
    """‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏ß‡∏•‡∏≤ fetch + ‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ ‡∏•‡∏á CSV ‡πÅ‡∏ö‡∏ö‡∏õ‡∏•‡∏≠‡∏î‡∏†‡∏±‡∏¢ ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏∑‡∏ô‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà fetch"""
    result = fetch_data()
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    new_data = pd.DataFrame([{
        "fetch_time": now,
        "status": result,
        "source": source
    }])

    old_data = safe_read_csv(CSV_FILE)
    df = pd.concat([old_data, new_data], ignore_index=True)
    df.to_csv(CSV_FILE, index=False, encoding="utf-8-sig")

    print(f"üìù Logged fetch at {now} | {source} | {result}")
    return now, result  # <-- ‡∏Ñ‡∏∑‡∏ô‡∏Ñ‡πà‡∏≤‡πÄ‡∏ß‡∏•‡∏≤‡∏ó‡∏µ‡πà fetch ‡πÄ‡∏™‡∏£‡πá‡∏à
